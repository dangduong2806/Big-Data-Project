[2025-11-22T17:34:25.701+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: training_pipeline.save_to_hdfs manual__2025-11-22T17:34:14.806147+00:00 [queued]>
[2025-11-22T17:34:25.709+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: training_pipeline.save_to_hdfs manual__2025-11-22T17:34:14.806147+00:00 [queued]>
[2025-11-22T17:34:25.709+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2025-11-22T17:34:25.719+0000] {taskinstance.py:2191} INFO - Executing <Task(DockerOperator): save_to_hdfs> on 2025-11-22 17:34:14.806147+00:00
[2025-11-22T17:34:25.724+0000] {standard_task_runner.py:60} INFO - Started process 232 to run task
[2025-11-22T17:34:25.726+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'training_pipeline', 'save_to_hdfs', 'manual__2025-11-22T17:34:14.806147+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/train.py', '--cfg-path', '/tmp/tmpsle6dvnr']
[2025-11-22T17:34:25.729+0000] {standard_task_runner.py:88} INFO - Job 7: Subtask save_to_hdfs
[2025-11-22T17:34:25.783+0000] {task_command.py:423} INFO - Running <TaskInstance: training_pipeline.save_to_hdfs manual__2025-11-22T17:34:14.806147+00:00 [running]> on host db2e79706642
[2025-11-22T17:34:25.847+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='training_pipeline' AIRFLOW_CTX_TASK_ID='save_to_hdfs' AIRFLOW_CTX_EXECUTION_DATE='2025-11-22T17:34:14.806147+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-11-22T17:34:14.806147+00:00'
[2025-11-22T17:34:25.919+0000] {docker.py:359} INFO - Starting docker container from image spark-jobs:latest
[2025-11-22T17:34:28.103+0000] {docker.py:429} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-11-22T17:34:28.169+0000] {docker.py:429} INFO - Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
[2025-11-22T17:34:28.173+0000] {docker.py:429} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-11-22T17:34:28.174+0000] {docker.py:429} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-cae3d8dd-afb5-4318-b5f4-1c6e5a6a2306;1.0
	confs: [default]
[2025-11-22T17:34:28.175+0000] {docker.py:429} INFO - 
[2025-11-22T17:34:30.311+0000] {docker.py:429} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
[2025-11-22T17:34:30.580+0000] {docker.py:429} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2025-11-22T17:34:30.768+0000] {docker.py:429} INFO - found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-11-22T17:34:30.960+0000] {docker.py:429} INFO - found org.lz4#lz4-java;1.8.0 in central
[2025-11-22T17:34:31.164+0000] {docker.py:429} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2025-11-22T17:34:32.339+0000] {docker.py:429} INFO - found org.slf4j#slf4j-api;2.0.7 in central
[2025-11-22T17:34:34.287+0000] {docker.py:429} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-11-22T17:34:34.433+0000] {docker.py:429} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-11-22T17:34:35.915+0000] {docker.py:429} INFO - found commons-logging#commons-logging;1.1.3 in central
[2025-11-22T17:34:36.038+0000] {docker.py:429} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-11-22T17:34:37.841+0000] {docker.py:429} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2025-11-22T17:34:37.904+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
[2025-11-22T17:34:38.050+0000] {docker.py:429} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (202ms)
[2025-11-22T17:34:38.101+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
[2025-11-22T17:34:38.160+0000] {docker.py:429} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (108ms)
[2025-11-22T17:34:38.213+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[2025-11-22T17:34:38.707+0000] {docker.py:429} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (547ms)
[2025-11-22T17:34:38.758+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2025-11-22T17:34:38.816+0000] {docker.py:429} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (108ms)
[2025-11-22T17:34:38.875+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2025-11-22T17:34:38.933+0000] {docker.py:429} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (116ms)
[2025-11-22T17:34:38.990+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2025-11-22T17:34:40.603+0000] {docker.py:429} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (1670ms)
[2025-11-22T17:34:40.656+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2025-11-22T17:34:40.762+0000] {docker.py:429} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (158ms)
[2025-11-22T17:34:40.816+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2025-11-22T17:34:40.976+0000] {docker.py:429} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (214ms)
[2025-11-22T17:34:41.079+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2025-11-22T17:34:41.135+0000] {docker.py:429} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (158ms)
[2025-11-22T17:34:41.195+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2025-11-22T17:34:42.109+0000] {docker.py:429} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (973ms)
[2025-11-22T17:34:42.168+0000] {docker.py:429} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2025-11-22T17:34:42.220+0000] {docker.py:429} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (110ms)
[2025-11-22T17:34:42.221+0000] {docker.py:429} INFO - :: resolution report :: resolve 9675ms :: artifacts dl 4372ms
[2025-11-22T17:34:42.221+0000] {docker.py:429} INFO - :: modules in use:
[2025-11-22T17:34:42.222+0000] {docker.py:429} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-11-22T17:34:42.222+0000] {docker.py:429} INFO - org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2025-11-22T17:34:42.223+0000] {docker.py:429} INFO - ---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
[2025-11-22T17:34:42.223+0000] {docker.py:429} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
[2025-11-22T17:34:42.224+0000] {docker.py:429} INFO - |      default     |   11  |   11  |   11  |   0   ||   11  |   11  |
	---------------------------------------------------------------------
[2025-11-22T17:34:42.228+0000] {docker.py:429} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-cae3d8dd-afb5-4318-b5f4-1c6e5a6a2306
	confs: [default]
[2025-11-22T17:34:42.282+0000] {docker.py:429} INFO - 11 artifacts copied, 0 already retrieved (56767kB/54ms)
[2025-11-22T17:34:42.526+0000] {docker.py:429} INFO - 25/11/22 17:34:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-11-22T17:34:42.735+0000] {docker.py:429} INFO - Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-11-22T17:34:44.819+0000] {docker.py:429} INFO - 2025-11-22 17:34:44,817 - INFO - Starting Spark Session creation...
2025-11-22 17:34:44,817 - INFO - Spark Session created successfully
2025-11-22 17:34:44,817 - INFO - Reading data from Kafka topic 'historical_prices'...
[2025-11-22T17:34:47.545+0000] {docker.py:429} INFO - 25/11/22 17:34:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:34:49.246+0000] {docker.py:429} INFO - [Stage 0:>                                                          (0 + 1) / 1]
[2025-11-22T17:34:52.614+0000] {docker.py:429} INFO - 
[2025-11-22T17:34:52.621+0000] {docker.py:429} INFO - 2025-11-22 17:34:52,619 - INFO - Read 250 records from Kafka
2025-11-22 17:34:52,619 - INFO - Parsing JSON data with schema...
[2025-11-22T17:34:52.846+0000] {docker.py:429} INFO - 25/11/22 17:34:52 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:34:53.448+0000] {docker.py:429} INFO - [Stage 3:>                                                          (0 + 1) / 1]
[2025-11-22T17:34:53.741+0000] {docker.py:429} INFO - 
[2025-11-22T17:34:53.744+0000] {docker.py:429} INFO - 2025-11-22 17:34:53,742 - INFO - Parsed 250 records successfully
2025-11-22 17:34:53,743 - INFO - Processing timestamps and creating partitions...
[2025-11-22T17:34:53.879+0000] {docker.py:429} INFO - 25/11/22 17:34:53 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:34:54.674+0000] {docker.py:429} INFO - 2025-11-22 17:34:54,672 - INFO - Processed 250 records with partitioning columns
2025-11-22 17:34:54,672 - INFO - Sample of processed data:
[2025-11-22T17:34:54.768+0000] {docker.py:429} INFO - 25/11/22 17:34:54 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:34:55.651+0000] {docker.py:429} INFO - [Stage 9:>                                                          (0 + 1) / 1]
[2025-11-22T17:34:55.768+0000] {docker.py:429} INFO - 
[2025-11-22T17:34:56.700+0000] {docker.py:429} INFO - 2025-11-22 17:34:56,699 - INFO - Writing data to HDFS...
[2025-11-22T17:34:57.182+0000] {docker.py:429} INFO - 25/11/22 17:34:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:34:58.055+0000] {docker.py:429} INFO - [Stage 10:>                                                         (0 + 1) / 1]
[2025-11-22T17:35:58.123+0000] {docker.py:429} INFO - [Stage 10:>                                                         (0 + 1) / 1]
[2025-11-22T17:36:12.429+0000] {docker.py:429} INFO - 
[2025-11-22T17:36:12.592+0000] {docker.py:429} INFO - 2025-11-22 17:36:12,591 - INFO - Data successfully written to HDFS
[2025-11-22T17:36:12.593+0000] {docker.py:429} INFO - 2025-11-22 17:36:12,592 - INFO - Starting Spark Session creation...
2025-11-22 17:36:12,592 - INFO - Spark Session created successfully
2025-11-22 17:36:12,592 - INFO - Reading data from Kafka topic 'historical_prices'...
[2025-11-22T17:36:12.621+0000] {docker.py:429} INFO - 25/11/22 17:36:12 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:36:13.435+0000] {docker.py:429} INFO - 2025-11-22 17:36:13,426 - INFO - Read 250 records from Kafka
[2025-11-22T17:36:13.440+0000] {docker.py:429} INFO - 2025-11-22 17:36:13,427 - INFO - Parsing JSON data with schema...
[2025-11-22T17:36:13.549+0000] {docker.py:429} INFO - 25/11/22 17:36:13 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:36:14.331+0000] {docker.py:429} INFO - 2025-11-22 17:36:14,330 - INFO - Parsed 250 records successfully
2025-11-22 17:36:14,330 - INFO - Processing timestamps and creating partitions...
[2025-11-22T17:36:14.426+0000] {docker.py:429} INFO - 25/11/22 17:36:14 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:36:15.139+0000] {docker.py:429} INFO - [Stage 17:>                                                         (0 + 1) / 1]
[2025-11-22T17:36:15.199+0000] {docker.py:429} INFO - 
[2025-11-22T17:36:15.203+0000] {docker.py:429} INFO - 2025-11-22 17:36:15,202 - INFO - Processed 250 records with partitioning columns
[2025-11-22T17:36:15.203+0000] {docker.py:429} INFO - 2025-11-22 17:36:15,202 - INFO - Sample of processed data:
[2025-11-22T17:36:15.389+0000] {docker.py:429} INFO - 25/11/22 17:36:15 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:36:16.139+0000] {docker.py:429} INFO - [Stage 20:>                                                         (0 + 1) / 1]
[2025-11-22T17:36:16.174+0000] {docker.py:429} INFO - 
[2025-11-22T17:36:16.182+0000] {docker.py:429} INFO - 2025-11-22 17:36:16,181 - INFO - Writing data to HDFS...
[2025-11-22T17:36:16.259+0000] {docker.py:429} INFO - 25/11/22 17:36:16 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:36:16.940+0000] {docker.py:429} INFO - [Stage 21:>                                                         (0 + 1) / 1]
[2025-11-22T17:37:17.019+0000] {docker.py:429} INFO - [Stage 21:>                                                         (0 + 1) / 1]
[2025-11-22T17:37:36.297+0000] {docker.py:429} INFO - 
[2025-11-22T17:37:36.327+0000] {docker.py:429} INFO - 2025-11-22 17:37:36,326 - INFO - Data successfully written to HDFS
2025-11-22 17:37:36,326 - INFO - Starting Spark Session creation...
2025-11-22 17:37:36,326 - INFO - Spark Session created successfully
2025-11-22 17:37:36,326 - INFO - Reading data from Kafka topic 'historical_prices'...
[2025-11-22T17:37:36.355+0000] {docker.py:429} INFO - 25/11/22 17:37:36 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:37:36.994+0000] {docker.py:429} INFO - 2025-11-22 17:37:36,992 - INFO - Read 250 records from Kafka
2025-11-22 17:37:36,992 - INFO - Parsing JSON data with schema...
[2025-11-22T17:37:37.024+0000] {docker.py:429} INFO - 25/11/22 17:37:37 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:37:37.664+0000] {docker.py:429} INFO - 2025-11-22 17:37:37,662 - INFO - Parsed 250 records successfully
2025-11-22 17:37:37,662 - INFO - Processing timestamps and creating partitions...
[2025-11-22T17:37:37.704+0000] {docker.py:429} INFO - 25/11/22 17:37:37 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:37:38.335+0000] {docker.py:429} INFO - 2025-11-22 17:37:38,333 - INFO - Processed 250 records with partitioning columns
2025-11-22 17:37:38,333 - INFO - Sample of processed data:
[2025-11-22T17:37:38.355+0000] {docker.py:429} INFO - 25/11/22 17:37:38 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:37:38.960+0000] {docker.py:429} INFO - 2025-11-22 17:37:38,958 - INFO - Writing data to HDFS...
[2025-11-22T17:37:38.983+0000] {docker.py:429} INFO - 25/11/22 17:37:38 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:37:39.638+0000] {docker.py:429} INFO - [Stage 32:>                                                         (0 + 1) / 1]
[2025-11-22T17:38:39.700+0000] {docker.py:429} INFO - [Stage 32:>                                                         (0 + 1) / 1]
[2025-11-22T17:38:50.586+0000] {docker.py:429} INFO - 
[2025-11-22T17:38:50.609+0000] {docker.py:429} INFO - 2025-11-22 17:38:50,608 - INFO - Data successfully written to HDFS
2025-11-22 17:38:50,608 - INFO - Starting Spark Session creation...
2025-11-22 17:38:50,608 - INFO - Spark Session created successfully
2025-11-22 17:38:50,608 - INFO - Reading data from Kafka topic 'historical_prices'...
[2025-11-22T17:38:50.629+0000] {docker.py:429} INFO - 25/11/22 17:38:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:38:51.269+0000] {docker.py:429} INFO - 2025-11-22 17:38:51,268 - INFO - Read 250 records from Kafka
2025-11-22 17:38:51,268 - INFO - Parsing JSON data with schema...
[2025-11-22T17:38:51.294+0000] {docker.py:429} INFO - 25/11/22 17:38:51 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:38:51.913+0000] {docker.py:429} INFO - 2025-11-22 17:38:51,912 - INFO - Parsed 250 records successfully
2025-11-22 17:38:51,912 - INFO - Processing timestamps and creating partitions...
[2025-11-22T17:38:51.949+0000] {docker.py:429} INFO - 25/11/22 17:38:51 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:38:52.511+0000] {docker.py:429} INFO - [Stage 39:>                                                         (0 + 1) / 1]
[2025-11-22T17:38:52.567+0000] {docker.py:429} INFO - 
[2025-11-22T17:38:52.573+0000] {docker.py:429} INFO - 2025-11-22 17:38:52,567 - INFO - Processed 250 records with partitioning columns
2025-11-22 17:38:52,567 - INFO - Sample of processed data:
[2025-11-22T17:38:52.584+0000] {docker.py:429} INFO - 25/11/22 17:38:52 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:38:53.196+0000] {docker.py:429} INFO - 2025-11-22 17:38:53,191 - INFO - Writing data to HDFS...
[2025-11-22T17:38:53.277+0000] {docker.py:429} INFO - 25/11/22 17:38:53 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-11-22T17:38:54.112+0000] {docker.py:429} INFO - [Stage 43:>                                                         (0 + 1) / 1]
[2025-11-22T17:39:54.167+0000] {docker.py:429} INFO - [Stage 43:>                                                         (0 + 1) / 1]
[2025-11-22T17:40:00.209+0000] {docker.py:429} INFO - 
[2025-11-22T17:40:00.232+0000] {docker.py:429} INFO - 2025-11-22 17:40:00,230 - INFO - Data successfully written to HDFS
[2025-11-22T17:40:00.232+0000] {docker.py:429} INFO - +-------+-------------------+----------------+----------------+----------------+---------------+----------+------------+------------------+-----------------------+----+-----+---+
|Company|timestamp          |Open            |High            |Low             |Close          |Volume    |Daily_Return|Volatility_Cluster|Volume_Based_Volatility|year|month|day|
+-------+-------------------+----------------+----------------+----------------+---------------+----------+------------+------------------+-----------------------+----+-----+---+
|GSPC   |2024-11-22 05:00:00|5944.35986328125|5972.89990234375|5944.35986328125|5969.33984375  |4141420000|NULL        |NULL              |NULL                   |2024|11   |22 |
|GSPC   |2024-11-25 05:00:00|5992.27978515625|6020.75         |5963.91015625   |5987.3701171875|5633150000|NULL        |NULL              |NULL                   |2024|11   |25 |
|GSPC   |2024-11-26 05:00:00|6000.02978515625|6025.419921875  |5992.27001953125|6021.6298828125|3835170000|NULL        |NULL              |NULL                   |2024|11   |26 |
|GSPC   |2024-11-27 05:00:00|6014.10986328125|6020.16015625   |5984.8701171875 |5998.740234375 |3363340000|NULL        |NULL              |NULL                   |2024|11   |27 |
|GSPC   |2024-11-29 05:00:00|6003.97998046875|6044.169921875  |6003.97998046875|6032.3798828125|2444420000|NULL        |NULL              |NULL                   |2024|11   |29 |
+-------+-------------------+----------------+----------------+----------------+---------------+----------+------------+------------------+-----------------------+----+-----+---+
only showing top 5 rows

+-------+-------------------+------------------+------------------+------------------+------------------+--------+------------+------------------+-----------------------+----+-----+---+
|Company|timestamp          |Open              |High              |Low               |Close             |Volume  |Daily_Return|Volatility_Cluster|Volume_Based_Volatility|year|month|day|
+-------+-------------------+------------------+------------------+------------------+------------------+--------+------------+------------------+-----------------------+----+-----+---+
|XOM    |2024-11-22 05:00:00|117.50732268648053|118.8481132563591 |117.33369476005677|117.47838592529297|13323400|NULL        |NULL              |NULL                   |2024|11   |22 |
|XOM    |2024-11-25 05:00:00|117.13112858177797|117.56519471266405|115.37556061885698|115.72281646728516|26580300|NULL        |NULL              |NULL                   |2024|11   |25 |
|XOM    |2024-11-26 05:00:00|115.29838764146885|115.44307880062742|113.67786284934868|113.79361724853516|14827300|NULL        |NULL              |NULL                   |2024|11   |26 |
|XOM    |2024-11-27 05:00:00|113.90936844246187|114.52671790133839|113.27273762196486|113.49459838867188|11079100|NULL        |NULL              |NULL                   |2024|11   |27 |
|XOM    |2024-11-29 05:00:00|113.28237840043553|114.30484980745092|112.80972013629255|113.78396606445312|9426500 |NULL        |NULL              |NULL                   |2024|11   |29 |
+-------+-------------------+------------------+------------------+------------------+------------------+--------+------------+------------------+-----------------------+----+-----+---+
only showing top 5 rows

+-------+-------------------+------------------+------------------+------------------+------------------+--------+------------+------------------+-----------------------+----+-----+---+
|Company|timestamp          |Open              |High              |Low               |Close             |Volume  |Daily_Return|Volatility_Cluster|Volume_Based_Volatility|year|month|day|
+-------+-------------------+------------------+------------------+------------------+------------------+--------+------------+------------------+-----------------------+----+-----+---+
|CVX    |2024-11-22 05:00:00|154.35459822797486|155.4058706317349 |154.01054676699675|155.1669464111328 |7005900 |NULL        |NULL              |NULL                   |2024|11   |22 |
|CVX    |2024-11-25 05:00:00|154.96624521964182|155.9410601327535 |152.74903601303205|153.25555419921875|10702400|NULL        |NULL              |NULL                   |2024|11   |25 |
|CVX    |2024-11-26 05:00:00|155.11915226618274|155.72124594380819|154.24946463911854|155.32940673828125|7369500 |NULL        |NULL              |NULL                   |2024|11   |26 |
|CVX    |2024-11-27 05:00:00|155.54922144087274|157.38415773005283|154.45017584430641|154.9280242919922 |7674500 |NULL        |NULL              |NULL                   |2024|11   |27 |
|CVX    |2024-11-29 05:00:00|155.06182672109688|155.53967518556865|153.54227210394325|154.75599670410156|5077000 |NULL        |NULL              |NULL                   |2024|11   |29 |
+-------+-------------------+------------------+------------------+------------------+------------------+--------+------------+------------------+-----------------------+----+-----+---+
only showing top 5 rows

+-------+-------------------+------------------+------------------+------------------+------------------+--------+------------+------------------+-----------------------+----+-----+---+
|Company|timestamp          |Open              |High              |Low               |Close             |Volume  |Daily_Return|Volatility_Cluster|Volume_Based_Volatility|year|month|day|
+-------+-------------------+------------------+------------------+------------------+------------------+--------+------------+------------------+-----------------------+----+-----+---+
|BP     |2024-11-22 05:00:00|27.650107499485223|28.046322015722716|27.59350593983162 |28.036888122558594|16067800|NULL        |NULL              |NULL                   |2024|11   |22 |
|BP     |2024-11-25 05:00:00|27.933121520555915|28.018024767678014|27.546340861146945|27.659543991088867|10472600|NULL        |NULL              |NULL                   |2024|11   |25 |
|BP     |2024-11-26 05:00:00|27.64067430124441 |27.64067430124441 |27.168990427166307|27.319929122924805|14119900|NULL        |NULL              |NULL                   |2024|11   |26 |
|BP     |2024-11-27 05:00:00|27.385965570745995|27.621807500844053|27.338796105127702|27.480300903320312|6882500 |NULL        |NULL              |NULL                   |2024|11   |27 |
|BP     |2024-11-29 05:00:00|27.593506776033472|27.678410017752377|27.489735747414542|27.650108337402344|4407500 |NULL        |NULL              |NULL                   |2024|11   |29 |
+-------+-------------------+------------------+------------------+------------------+------------------+--------+------------+------------------+-----------------------+----+-----+---+
only showing top 5 rows
[2025-11-22T17:40:00.543+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=training_pipeline, task_id=save_to_hdfs, execution_date=20251122T173414, start_date=20251122T173425, end_date=20251122T174000
[2025-11-22T17:40:00.570+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2025-11-22T17:40:00.586+0000] {taskinstance.py:3280} INFO - 1 downstream tasks scheduled from follow-on schedule check
